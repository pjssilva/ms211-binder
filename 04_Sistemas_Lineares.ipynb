{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using LinearAlgebra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sistemas Lineares\n",
    "\n",
    "Um dos problemas mais clássicos em computação científica é a resolução de sistemas lineares. Isso se dá por, pelo menos, dois motivos:\n",
    "\n",
    "1. Sabemos resolvê-los bem. Ou seja, temos confiança que conseguimos resolver com alta precisão e de forma eficiente sistemas lineares com um número enorme de variáveis.\n",
    "\n",
    "1. Outros problemas importantes podem ser reduzidos à solução de um ou mais sistemas lineares (potencialmente grandes, potencialmente infinitos).\n",
    "\n",
    "Por isso, vamos estudá-los durante as próximas aulas. Vamos começar estabelecendo um pouco de notação e recordando o que já sabemos sobre a solução de sistemas lineares.\n",
    "\n",
    "Lembramos inicialmente que qualquer sistema linear de $n$ variáveis e $n$ incógnitas pode ser descrito em forma matricial. Ou seja, podemos descrever o problema de resolução de sistemas por uma equação.\n",
    "\\begin{equation*}\n",
    "Ax = b,\n",
    "\\end{equation*}\n",
    "em que $A$ é uma matriz de $\\mathbb{R}^{n \\times n}$ e $b$ um vetor do $\\mathbb{R}^n$. É claro que também é possível usar o mesmo tipo de notação para sistemas com números diferentes entre variáveis e incógnitas e nesse caso a matriz deixa de ser quadrada.\n",
    "\n",
    "Uma primeira pergunta natural é: Quão fácil é resolver um sistema? A resposta para isso não é única. Pensando em um sistema geral, com centenas de variáveis por exemplo, ficamos com a impressão que é muito difícil. Mas se a matriz $A$ tiver estrutura especial, pode ser que a resolução do sistema seja fácil. Dois exemplos são:\n",
    "\n",
    "* $A$ é a identidade. Nesse caso o solução é trivial $x = b$.\n",
    "* $A$ é diagonal (todos os elementos fora da diagonal principal de $A$ são nulos). Mais uma vez a solução do sistema também é fácil, basta definir $x_i = b_i / a_{ii},\\ i = 1, \\ldots, n$, considerando que todos os elementos da diagonal são não nulos.\n",
    "   \n",
    "## Sistemas triangulares\n",
    "   \n",
    "Outro caso interessante ocorre quando $A$ é triangular inferior, ou superior, com todos os elementos da diagonal não nulos. Nesse caso podemos resolver o sistema por substituição. Vejamos um exemplo triangular superior.\n",
    "\n",
    "\\begin{equation*}\n",
    "\\left\\{\n",
    "\\begin{array}{lclclcll}\n",
    "a_{11}x_1 &+& a_{12}x_2 &+& a_{13}x_3 &+& a_{14}x_4 &= b_1 \\\\\n",
    "          & & a_{22}x_2 &+& a_{23}x_3 &+& a_{24}x_4 &= b_2 \\\\\n",
    "          & &           & & a_{33}x_3 &+& a_{34}x_4 &= b_3 \\\\\n",
    "          & &           & &           & & a_{44}x_4 &= b_4. \n",
    "\\end{array}\n",
    "\\right.\n",
    "\\end{equation*}\n",
    "\n",
    "Nesse caso é fácil achar o valor de $x_4$ usando a última equação. Susbtituindo o valor encontrado na penúltima equação passa a ser fácil encontrar encontrar o valor de $x_3$ e a assim por diante. Obtemos:\n",
    "\n",
    "\\begin{align*}\n",
    "x_4 &= b_4 / a_{44} \\\\\n",
    "x_3 &= (b_3 - a_{34} x_4)/a_{33} \\\\\n",
    "x_2 &= (b_2 - a_{23} x_3 - a_{24} x_4)/a_{22} \\\\\n",
    "x_1 &= (b_1 - a_{12}x_2 - a_{13} x_3 - a_{14} x_4)/a_{11}. \\\\\n",
    "\\end{align*}\n",
    "\n",
    "Isso pode ser facilmente generalizado para o caso de mais variáveis. Chegamos à fórmula\n",
    "\\begin{equation}\n",
    "x_i = \\frac{b_i - \\sum_{j = i + 1}^n a_{ij} x_j}{a_{ii}},\\ i = 1, \\ldots, n.\n",
    "\\end{equation}\n",
    "\n",
    "Agora, o curso de Cálculo Numérico não é apenas um curso de Matemática, então para nós não basta deduzir as fórmulas, temos que pensar um pouco como implementá-las. Vamos então apresentar uma rotina simples que resolve um sistema por substituição (regressiva)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funcao para resolver sistema triangular superior por substituicao regressiva\n",
    "function subs_reg(A, b)\n",
    "    n = length(b)\n",
    "    x = Vector{Float64}(undef, n)\n",
    "    for i = n:-1:1\n",
    "        ld = b[i]\n",
    "        for j = i + 1:n\n",
    "            ld -= A[i, j]*x[j]\n",
    "        end\n",
    "        x[i] = ld / A[i, i]\n",
    "    end\n",
    "    return x\n",
    "end\n",
    "\n",
    "# Dados para teste\n",
    "A = [1.0 2 3; 0 3 7; 0 0 7]\n",
    "b = [1.0, 4, 3.2]\n",
    "\n",
    "# Resolve o sistema\n",
    "sol = subs_reg(A, b)\n",
    "println(\"Solucao obtida é \", sol)\n",
    "\n",
    "# Verifica a resposta\n",
    "println(\"A*sol = $(A*sol). Deveria ser igual a $b.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um exercício interessante é escrever a versão do código para resolução de sistemas que são triangulares inferiores. Nesse caso a variável da solução que pode ser obtida imediatamente é $x_1$ e não mais $x_n$. A partir dela se obtém $x_2$ continuando até $x_n$. Que tal fazer isso na caixa de programa abaixo?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tente implementar e testar uma rotina de substituicao progressiva \n",
    "# (para sistemas triangulares inferiores) aqui e faca alguns testes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sistemas gerais\n",
    "\n",
    "Agora o que fazer quando nos deparamos com um sistema geral, como abaixo?\n",
    "\n",
    "\\begin{equation}\n",
    "\\left\\{\n",
    "\\begin{array}{lclclcll}\n",
    "2x_1 &+& 3x_2 &+& x_3  &+& x_4  &= 3 \\\\\n",
    "4x_1 &+& 7x_2 &+& 4x_3 &+& 3x_4 &= 6 \\\\\n",
    "4x_1 &+& 7x_2 &+& 6x_3 &+& 4x_4 &= 4 \\\\\n",
    "6x_1 &+& 9x_2 &+& 9x_3 &+& 8x_4 &= 3.\n",
    "\\end{array}\n",
    "\\right.\n",
    "\\end{equation}\n",
    "\n",
    "Uma abordagem, que é ensinada no ensino médio, é a ideia de *escalonamento*. Busca-se introduzir zeros nos coeficientes da primeira coluna das equações 2 em diante (para baixo). Depois, transformamos em zeros os coeficientes da segunda coluna da equação 3 para frente e assim sucessivamente. O objetivo, é claro, é obter um sistema triangular equivalente ao sistema original. Para isso precisamos introduzir os zeros usando operações que preservem as soluções do sistema original. Isso pode ser feito usando-se operações elementares.\n",
    "\n",
    "### Operações elementares\n",
    "\n",
    "1. Multiplicar uma equação por um escalar não nulo.\n",
    "1. Trocar duas equações de lugar.\n",
    "1. Somar/Subtrair uma equação a uma outra.\n",
    "\n",
    "Por exemplo, podemos introduzir um zero no primeiro coeficiente da segunda equação multiplicando a primeira linha por $-2$ e somando o resultado à segunda. Obteríamos então o sistema:\n",
    "\\begin{equation}\n",
    "\\left\\{\n",
    "\\begin{array}{lclclcll}\n",
    "2x_1 &+& 3x_2 &+& x_3  &+& x_4  &= 3 \\\\\n",
    "     &+&  x_2 &+& 2x_3 &+& x_4  &= 0 \\\\\n",
    "4x_1 &+& 7x_2 &+& 6x_3 &+& 4x_4 &= 4 \\\\\n",
    "6x_1 &+& 9x_2 &+& 9x_3 &+& 8x_4 &= 3.\n",
    "\\end{array}\n",
    "\\right.\n",
    "\\end{equation}\n",
    "\n",
    "Outro fato interessante, que pode nos aproximar de uma implementação, é que as variáveis aparecem acima apenas por comodidade ou costume. A única informação que é realmente importante são os coeficientes e o lado direito. Isso sugere, mais uma vez, usar uma representação matricial do sistema. O sistema original poderia, então, ser representado por:\n",
    "\\begin{equation}\n",
    "\\left[\n",
    "\\begin{array}{llll|l}\n",
    "2 & 3 & 1 & 1 & 3 \\\\\n",
    "4 & 7 & 4 & 3 & 6 \\\\\n",
    "4 & 7 & 6 & 4 & 4 \\\\\n",
    "6 & 9 & 9 & 8 & 3.\n",
    "\\end{array}\n",
    "\\right].\n",
    "\\end{equation}\n",
    "\n",
    "Já o sistema transformado, após colocar um zero na posição $(2, 1)$, seria\n",
    "\\begin{equation}\n",
    "\\left[\n",
    "\\begin{array}{llll|l}\n",
    "2 & 3 & 1 & 1 & 3 \\\\\n",
    "0 & 1 & 2 & 1 & 0 \\\\\n",
    "4 & 7 & 6 & 4 & 4 \\\\\n",
    "6 & 9 & 9 & 8 & 3.\n",
    "\\end{array}\n",
    "\\right].\n",
    "\\end{equation}\n",
    "\n",
    "É claro que podemos colocar zeros também nas posições 3 e 4 da primeira coluna. Basta somar à terceira linha $-2$ vezes a primeira. O coeficiente usado na multiplicação é obtido pegando o negativo do valor da posição que se quer zerar e dividindo pelo elemento que está na posição $1 \\times 1$. Já para colocar um zero na posição $(4, 1)$, devemos somar à quarta $-3$ vezes a primeira. Mais uma vez, o coeficiente $-3$ é obtido através da divisão entre o negativo do número que está na posição que deve ser eliminada dividido pelo número que está em $(1, 1)$. Vamos fazer isso, mas usando Julia. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dados do sistema\n",
    "A = [2.0 3 1 1; 0 1 2 1; 4 7 6 4; 6 9 9 8]\n",
    "b = [3.0, 0, 4, 3]\n",
    "\n",
    "println(\"Sistema\")\n",
    "sistema = [A b]     # Aqui concatenamos a matriz A e o vetor b que fica mais à direita"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Introduzindo zero na posicao (3, 1)\n",
    "sistema[3, :] = sistema[3, :] - sistema[3,1]/sistema[1,1] * sistema[1, :]\n",
    "\n",
    "println(\"Sistema\")\n",
    "sistema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Introduzindo zero na posicao (4, 1)\n",
    "sistema[4, :] .-= sistema[4, 1]/sistema[1,1]*sistema[1, :]\n",
    "\n",
    "println(\"Sistema:\")\n",
    "sistema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assim, ao terminarmos de colocar zeros na primeira coluna abaixo da posição $(1, 1)$, obtemos\n",
    "\\begin{equation}\n",
    "\\left[\n",
    "\\begin{array}{rrrr|r}\n",
    "2 & 3 & 1 & 1 & 3 \\\\\n",
    "0 & 1 & 2 & 1 & 0 \\\\\n",
    "0 & 1 & 4 & 2 & -2 \\\\\n",
    "0 & 0 & 6 & 5 & -6\n",
    "\\end{array}\n",
    "\\right].\n",
    "\\end{equation}\n",
    "\n",
    "O próximo passo para se obter um sistema triangular é colocar zeros abaixo da posição $(2, 2)$. Isso pode ser obtido somando à terceira linha o inverso da segunda. Por sorte, o elemento na posição $(4, 2)$ já é zero e não precisa ser modificado. Fazemos então:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Introduzindo zero na posicao (3, 2)\n",
    "sistema[3,:] .-= sistema[3, 2]/sistema[2, 2]*sistema[2, :]\n",
    "\n",
    "println(\"Sistema:\")\n",
    "sistema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Terminamos o processo introduzindo um zero na posição $(4, 3)$ usando um múltiplo da linha $3$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Introduzindo zero na posicao (4, 3)\n",
    "sistema[4, :] .-= sistema[4, 3]/sistema[3, 3]*sistema[3, :]\n",
    "\n",
    "println(\"Sistema:\")\n",
    "sistema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ao final, temos o sistema\n",
    "\\begin{equation}\n",
    "\\left[\n",
    "\\begin{array}{rrrr|r}\n",
    "2 & 3 & 1 & 1 & 3 \\\\\n",
    "0 & 1 & 2 & 1 & 0 \\\\\n",
    "0 & 0 & 2 & 1 & -2 \\\\\n",
    "0 & 0 & 0 & 2 & 0\n",
    "\\end{array}\n",
    "\\right].\n",
    "\\end{equation}\n",
    "Ou, usando a notação mais usual,\n",
    "\\begin{equation}\n",
    "\\left\\{\n",
    "\\begin{array}{rcrcrcrcl}\n",
    "2x_1 & + & 3x_2 & + & 1x_3 & + & 1x_4 & = & 3 \\\\\n",
    "     & + & 1x_2 & + & 2x_3 & + & 1x_4 & = & 0 \\\\\n",
    "     &   &      &   & 2x_3 & + & 1x_4 & = & -2 \\\\\n",
    "     &   &      &   &      &   & 2x_4 & = & 0.\n",
    "\\end{array}\n",
    "\\right.\n",
    "\\end{equation}\n",
    "Esse é um sistema triangular superior que está pronto para ser resolvido por substituição regressiva, apresentada acima. É claro que podemos escrever um programa em Julia que faz tudo de uma vez, pegando um sistema original e devolvendo a versão escalonada.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escalonamento em Julia\n",
    "function escalonamento(A, b)\n",
    "    n = length(b)\n",
    "    sistema = [A b] # Isso gera uma cópia\n",
    "    # Coloca zeros abaixo da posicao i x i usando a linha i\n",
    "    for i = 1:n - 1\n",
    "        # Atualiza o restante da matriz a partir da linha e coluna i + 1\n",
    "        for j = i + 1:n\n",
    "            coef = -sistema[j, i] / sistema[i, i]\n",
    "            sistema[j, i] = 0.0\n",
    "            sistema[j, i + 1:end] .+= coef .* sistema[i, i + 1:end]\n",
    "        end\n",
    "    end\n",
    "    return sistema\n",
    "end\n",
    "\n",
    "A = [2.0 3 1 1; 4 7 4 3; 4 7 6 4; 6 9 9 8]\n",
    "b = [3.0, 6, 4, 3]\n",
    "print(\"Sistema escalonado:\")\n",
    "escalonamento(A, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veja que o sistema obtido ao final é examente o sistema desejado.\n",
    "\n",
    "O código acima exige alguns comentários para explicar o que ocorre no laço `for j`. Veja que iniciamos colocando os zeros que sabemos que estarão lá no final, afinal de contas o objtivo do laço `for j` é colocar zeros abaixo da diagonal da coluna `i`. Depois iniciamos o processo de escalonamento andando linha-por-linha. Calculamos o coeficiente certo, multiplicamos pela linha i e somamos na linha de baixo. Também tomamos cuidado de não somar o monte de zeros que estão na submatriz `sistema[i+1:end, 1:i]`, já que sabemos que multiplicar 0 por um coeficiente qualquer e somar com 0 dá 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fatoração LU\n",
    "\n",
    "Se $A$ é uma matriz, uma *fatoração* de $A$ é um produto de outras matrizes, geralmente mais simples, que resulta em $A$. Isso é análogo ao caso de números inteiros que são fatorados em números primos.\n",
    "\n",
    "Um fato interessante sobre o processo de escalonamento é que ele encontra, implicitamente, uma fatoração especial de $A$. Se organizarmos as contas do escalonamento, podemos mostrar que ele encontra uma matriz triangular inferior $L$ e outra superior $U$ tal que\n",
    "$$\n",
    "A = LU.\n",
    "$$\n",
    "\n",
    "Para vermos que isso é verdade, vamos analisar como representar cada operação realizada pelo escalonamento através de produtos de matrizes. Com esse objetivo, vamos pensar o que ocorre quando multiplicamos uma matriz $A$ à esquerda por uma outra matriz que é igual à identidade a menos de uma posição fora da diagonal. Por exemplo\n",
    "$$\n",
    "\\left( \\begin{array}{rrrr}\n",
    "1 & 0 & 0 & 0 \\\\\n",
    "0 & 1 & 0 & 0 \\\\\n",
    "-2 & 0 & 1 & 0 \\\\\n",
    "0 & 0 & 0 & 1\n",
    "\\end{array} \\right)\n",
    "\\left( \\begin{array}{rrrr}\n",
    "2 & 3 & 1 & 1 \\\\\n",
    "4 & 7 & 4 & 3 \\\\\n",
    "4 & 7 & 6 & 4 \\\\\n",
    "6 & 9 & 9 & 8\n",
    "\\end{array} \\right).\n",
    "$$\n",
    "Trabalhando um pouco nesse exemplo você verá que a matriz resultante é igual a matriz da direita com a terceira linha trocada pela linha que estava lá menos 2 vezes a primeira linha, ou seja\n",
    "$$\n",
    "\\left( \\begin{array}{rrrr}\n",
    "2 & 3 & 1 & 1 \\\\\n",
    "4 & 7 & 4 & 3 \\\\\n",
    "0 & 1 & 4 & 2 \\\\\n",
    "6 & 9 & 9 & 8\n",
    "\\end{array} \\right).\n",
    "$$\n",
    "De uma maneira geral, se multiplicarmos uma matriz $A$ à esquerda por uma matriz igual à identidade a menos de um elemento $c$ na posição $(i, j)$, obteremos uma matriz igual com a linha $i$ trocada por ela mesma somada a $c$ vezes a linha $j$. Esse é exatamente o tipo de operação usada pelo escalonamento. Ele pode ser interpretada como uma sequência de operações elementares do tipo multiplicar linhas por um número não nulo que são somadas a outras linhas subtituindo o que estava lá. Logo o processo de escalonamento pode ser representada por uma sequência dde multiplicações por matrizes. Por exemplo, para preencher com zeros a primeira coluna da matriz original acima, começando pela posição $(2, 1)$ até a posição $(4, 1)$ bastaria considerar os seguintes produtos por matrizes (leia da esquerda para a direita para entender mais facilmente).\n",
    "$$\n",
    "\\underbrace{\n",
    "\\left( \\begin{array}{rrrr}\n",
    "1 & 0 & 0 & 0 \\\\\n",
    "0 & 1 & 0 & 0 \\\\\n",
    "0 & 0 & 1 & 0 \\\\\n",
    "-3 & 0 & 0 & 1\n",
    "\\end{array} \\right)}_{L_{41}}\n",
    "\\underbrace{\n",
    "\\left( \\begin{array}{rrrr}\n",
    "1 & 0 & 0 & 0 \\\\\n",
    "0 & 1 & 0 & 0 \\\\\n",
    "-2 & 0 & 1 & 0 \\\\\n",
    "0 & 0 & 0 & 1\n",
    "\\end{array} \\right)}_{L_{31}}\n",
    "\\underbrace{\n",
    "\\left( \\begin{array}{rrrr}\n",
    "1 & 0 & 0 & 0 \\\\\n",
    "-2 & 1 & 0 & 0 \\\\\n",
    "0 & 0 & 1 & 0 \\\\\n",
    "0 & 0 & 0 & 1\n",
    "\\end{array} \\right)}_{L_{21}}\n",
    "\\underbrace{\n",
    "\\left( \\begin{array}{rrrr}\n",
    "2 & 3 & 1 & 1 \\\\\n",
    "4 & 7 & 4 & 3 \\\\\n",
    "4 & 7 & 6 & 4 \\\\\n",
    "6 & 9 & 9 & 8\n",
    "\\end{array} \\right)}_{A} = \n",
    "\\left( \\begin{array}{rrrr}\n",
    "2 & 3 & 1 & 1 \\\\\n",
    "0 & 1 & 2 & 1 \\\\\n",
    "0 & 1 & 4 & 2 \\\\\n",
    "0 & 0 & 6 & 5\n",
    "\\end{array} \\right).\n",
    "$$\n",
    "A notacão $L_{ij}$ representa matriz que coloca zero na posição $(i, j)$. De fato vamos verificar isso computacionalmente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = [2.0 3 1 1; 4 7 4 3; 4 7 6 4; 6 9 9 8]\n",
    "L21 = Matrix{Float64}(I, 4, 4)\n",
    "L21[2, 1] = -2\n",
    "L31 = Matrix{Float64}(I, 4, 4)\n",
    "L31[3, 1] = -2\n",
    "L41 = Matrix{Float64}(I, 4, 4)\n",
    "L41[4, 1] = -3\n",
    "\n",
    "L41*(L31*(L21*A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare o resultado obtido com o resultado do processo de escalonamento depois de colocar zeros abaixo do primeiro elemento da diagonal acima. Veja que a matriz de partida é a mesma. Outro fato interessante sobre as matrizes $L_1,\\ L_2,\\ L_3$ acima é que o seu produto é particularmente simples.\n",
    "$$\n",
    "\\underbrace{\n",
    "\\left( \\begin{array}{rrrr|r}\n",
    "1 & 0 & 0 & 0 \\\\\n",
    "0 & 1 & 0 & 0 \\\\\n",
    "0 & 0 & 1 & 0 \\\\\n",
    "-3 & 0 & 0 & 1\n",
    "\\end{array} \\right)}_{L_{41}}\n",
    "\\underbrace{\n",
    "\\left( \\begin{array}{rrrr|r}\n",
    "1 & 0 & 0 & 0 \\\\\n",
    "0 & 1 & 0 & 0 \\\\\n",
    "-2 & 0 & 1 & 0 \\\\\n",
    "0 & 0 & 0 & 1\n",
    "\\end{array} \\right)}_{L_{31}}\n",
    "\\underbrace{\n",
    "\\left( \\begin{array}{rrrr|r}\n",
    "1 & 0 & 0 & 0 \\\\\n",
    "-2 & 1 & 0 & 0 \\\\\n",
    "0 & 0 & 1 & 0 \\\\\n",
    "0 & 0 & 0 & 1\n",
    "\\end{array} \\right)}_{L_{21}} =\n",
    "\\underbrace{\n",
    "\\left( \\begin{array}{rrrr|r}\n",
    "1 & 0 & 0 & 0 \\\\\n",
    "-2 & 1 & 0 & 0 \\\\\n",
    "-2 & 0 & 1 & 0 \\\\\n",
    "-3 & 0 & 0 & 1\n",
    "\\end{array} \\right)}_{L_1}.\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basta copiar abaixo da primeira diagonal da identidade os coeficientes que aparecem nas três matrizes na sua posição natural, formando assim a matriz que zera os elementos desejados da coluna $1$. Vamos chamar essa matriz de $L_1$. Para entender o porque isso acontece basta interpretar o produto da direita para a esquerda. Podemos mais uma vez confirmar isso computacionalmente. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L1 = L41*L31*L21"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outro fato interessante sobre as matrizes que se combinam para formar $L_1$ é que suas inversas são fáceis de calcular. Basta inverter o elemento de fora da diagonal. Por exemplo\n",
    "$$\n",
    "L_{31}^{-1} = \n",
    "\\left( \\begin{array}{rrrr|r}\n",
    "1 & 0 & 0 & 0 \\\\\n",
    "0 & 1 & 0 & 0 \\\\\n",
    "2 & 0 & 1 & 0 \\\\\n",
    "0 & 0 & 0 & 1\n",
    "\\end{array} \\right).\n",
    "$$\n",
    "Confirmando:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L31⁻¹ = Matrix{Float64}(I, 4, 4)\n",
    "L31⁻¹[3, 1] = 2\n",
    "\n",
    "L31⁻¹*L31"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isso pode ser entendido porque essa mudança de sinal desfaz a operação elementar representada por essas matrizes. Pense um pouco. \n",
    "\n",
    "Agora, lembrando que o inverso de um produto é o produto em ordem invertida das inversas termos\n",
    "$$\n",
    "L_1^{-1} = (L_{41} L_{31} L_{21})^{-1} =  L_{21}^{-1} L_{31}^{-1} L_{41}^{-1} = \n",
    "\\left( \\begin{array}{rrrr|r}\n",
    "1 & 0 & 0 & 0 \\\\\n",
    "2 & 1 & 0 & 0 \\\\\n",
    "2 & 0 & 1 & 0 \\\\\n",
    "3 & 0 & 0 & 1\n",
    "\\end{array} \\right).\n",
    "$$\n",
    "De novo ocorre o fenômeno em que é simples multiplicar essas matrizes: basta copiar os elementos adequados nas respostas.\n",
    "\n",
    "O restante do processo de escalonamento também pode continuar sendo representado por multiplicação por matrizes desse tipo. Continuamos o processo colocando zeros abaixo do segundo elemento da diagonal e por fim abaixo do terceiro. O processo todo seria então representado pelo produto.\n",
    "$$\n",
    "\\underbrace{\n",
    "\\underbrace{\n",
    "\\left( \\begin{array}{rrrr}\n",
    "1 & 0 & 0 & 0 \\\\\n",
    "0 & 1 & 0 & 0 \\\\\n",
    "0 & 0 & 1 & 0 \\\\\n",
    "0 & 0 & -3 & 1\n",
    "\\end{array} \\right)}_{L_{43}}\n",
    "}_{L_3}\n",
    "\\underbrace{\n",
    "\\underbrace{\n",
    "\\left( \\begin{array}{rrrr}\n",
    "1 & 0 & 0 & 0 \\\\\n",
    "0 & 1 & 0 & 0 \\\\\n",
    "0 & 0 & 1 & 0 \\\\\n",
    "0 & 0 & 0 & 1\n",
    "\\end{array} \\right)}_{L_{42}}\n",
    "\\underbrace{\n",
    "\\left( \\begin{array}{rrrr}\n",
    "1 & 0 & 0 & 0 \\\\\n",
    "0 & 1 & 0 & 0 \\\\\n",
    "0 & -1 & 1 & 0 \\\\\n",
    "0 & 0 & 0 & 1\n",
    "\\end{array} \\right)}_{L_{32}}\n",
    "}_{L_2}\n",
    "L_1\n",
    "\\underbrace{\n",
    "\\left( \\begin{array}{rrrr}\n",
    "2 & 3 & 1 & 1 \\\\\n",
    "4 & 7 & 4 & 3 \\\\\n",
    "4 & 7 & 6 & 4 \\\\\n",
    "6 & 9 & 9 & 8\n",
    "\\end{array} \\right)}_{A} = \n",
    "\\left( \\begin{array}{rrrr}\n",
    "2 & 3 & 1 & 1 \\\\\n",
    "0 & 1 & 2 & 1 \\\\\n",
    "0 & 0 & 2 & 1 \\\\\n",
    "0 & 0 & 0 & 2\n",
    "\\end{array} \\right).\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L2 = Matrix{Float64}(I, 4, 4)\n",
    "L2[3, 2] = -1\n",
    "L2[4, 2] = 0\n",
    "L3 = Matrix{Float64}(I, 4, 4)\n",
    "L3[4, 3] = -3\n",
    "\n",
    "(L3*(L2*(L1*A)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, vamos partir dessa última equação. Temos\n",
    "$$\n",
    "L_3 L_2 L_1 A = U,\n",
    "$$\n",
    "em que $U$ é a matriz triangular superior que obtemos ao final do processo de escalonamento. Podemos então passar o produto das matrizes $L_k$ para o outro lado, usando suas inversas, que sabemos como calcular. Obtendo:\n",
    "$$\n",
    "A = L_1^{-1} L_2^{-1} L_3^{-1} U.\n",
    "$$\n",
    "Mais uma vez há um \"golpe de sorte\" e o produto $L_1^{-1} L_2^{-1} L_3^{-1}$ também tem uma expressão particularmente simples, basta copiar os elementos que aparecem abaixo das diagonais de cada uma dessa matrizes na resposta. Isso ocorre porque o produto $L_{k}^{-1} \\dots L_{n - 1}^{-1}$ é composto por matrizes que têm zeros abaixo da diagonal na coluna $k - 1$, já que nessas posições sempre houve $0$ nas linhas que já foram combinadas. Agora quando se multiplica essa matriz intermediária à esquerda por $L_{k-1}$ na coluna $k - 1$ apenas os elementos de $L_{k - 1}$ que estão estão abaixo da diagonal na coluna $k - 1$ irão importar, sendo diretamente copiados. Faça uns exemplos manualmente para se convencer disso. Já a confirmação numérica podemos fazer agora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv(L3*L2*L1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De fato a inversa obtida também é uma matriz triangular inferior com os elementos das matrizes inversas copiados. A essa matriz $L_1^{-1} L_2^{-1} \\ldots L_{n - 1}^{-1}$ vamos chamar de matriz $L$. Dessa forma obtermos justamente a expressão desejada:\n",
    "$$\n",
    "A = LU.\n",
    "$$\n",
    "Não podemos deixar de observar que talvez fosse mais natural nomear a matriz $L_1^{-1} L_2^{-1} \\ldots L_{n - 1}^{-1}$ de $L^{-1}$. Isso não é feito para evitar uma notação pesada na importante igualdade $A = LU$. \n",
    "\n",
    "Deste modo fica natural adaptar o código de escalonamento para calcular a fatoração LU de uma matriz. Para isso basta guardar os coeficientes usados, com sinal adequado, na matriz $L$. A matriz $U$ é a matriz triangular superior final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fatoracao LU de uma matriz A sem pivoteamento\n",
    "function preLU(A)\n",
    "    # Reserva espaco para a resposta\n",
    "    n, _ = size(A)\n",
    "    L = one(A)\n",
    "    U = copy(A)\n",
    "    # Coloca zeros abaixo da posicao i x i usando a linha i\n",
    "    for i = 1:n - 1\n",
    "        for j = i + 1:n\n",
    "            coef = U[j, i] / U[i, i]\n",
    "            L[j, i] = coef\n",
    "            U[j, i] = 0.0\n",
    "            U[j, i + 1:end] .-= coef .* U[i, i + 1:end]\n",
    "        end\n",
    "    end\n",
    "    return L, U\n",
    "end\n",
    "\n",
    "A = [2.0 3 1 1; 4 7 4 3; 4 7 6 4; 6 9 9 8]\n",
    "b = [3.0, 6, 4, 3]\n",
    "L, U = preLU(A);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L*U"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vejam que de fato $LU$ é igual à matriz $A$ original. Na verdade, essa igualdade absoluta é obtida nesse caso por que todas as contas, apesar de serem feitas com ponto flutante, geraram números inteiros que podem ser representados. De uma forma geral esperamos que o resultado final sejam aproximadamente a $L$ e a $U$ desejadas. Voltaremos a discutir erros depois.\n",
    "\n",
    "## Revisão parou aqui\n",
    "\n",
    "## Utilizando a fatoração LU para resolver sistemas\n",
    "\n",
    "Começamos essa discussão falando sobre a solução de sistemas lineares mas fizemos um desvio acima para comentar sobre uma fatoração de matrizes. Vamos agora voltar a trilha original, vendo como usar a fatoração LU para resolver um sistema linear. Desejamos encontrar $x$ tal que\n",
    "$$\n",
    "Ax = b.\n",
    "$$\n",
    "Isso é equivalente a buscar $x$ tal que\n",
    "$$\n",
    "LUx = b.\n",
    "$$\n",
    "Como lidar com sistemas que tem produtos de matrizes como acima? Uma ideia é criar variáveis intermediárias, resolvendo o sistema da esquerda para a direita. Por exemplo, no caso acima podemos inicialmente buscar $y$ tal que\n",
    "$$\n",
    "L y = b.\n",
    "$$\n",
    "Isso é fácil de calcular porque o sistema é triangular inferior. Depois disso resolvemos\n",
    "$$\n",
    "U x = y.\n",
    "$$\n",
    "Assim, obtemos o $x$ desejado, solução do sistema original. Note que de novo basta resolver um sistema triangular, mas agora superior. De fato colocando as duas últimas expressões juntas temos\n",
    "$$\n",
    "U x = y \\implies L (U x) = b \\implies Ax = b.\n",
    "$$\n",
    "A primeira implicação usa a definição de $y$. Mais uma vez, podemos ver isso funcionando usando o código que já desenvolvemos até aqui. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alem da rotina de substituicao regressiva para a resolucao de sistemas\n",
    "# triangulares superiores precisamos de uma outra que faca a resolucao\n",
    "# de sistemas triangulares inferiores. Isso tinha ficado como exercicio\n",
    "# antes e agora voces podem ver uma resposta.\n",
    "function subs_prog(A, b)\n",
    "    n = length(b)\n",
    "    x = Vector{Float64}(undef, n)\n",
    "    x[1] = b[1]/A[1, 1]\n",
    "    for i = 2:n\n",
    "        x[i] = (b[i] - dot(A[i, 1:i], x[1:i]))/A[i, i]\n",
    "    end\n",
    "    return x\n",
    "end\n",
    "\n",
    "# Por fim, usamos as rotinas implementadas até agora para resolver um \n",
    "# sistema linear.\n",
    "A = [2.0 3 1 1; 4 7 4 3; 4 7 6 4; 6 9 9 8]\n",
    "b = [3.0, 6, 4, 3]\n",
    "L, U = preLU(A)\n",
    "y = subs_prog(L, b)\n",
    "x = subs_reg(U, y)\n",
    "println(\"Solucao         = \", x)\n",
    "println(\"Verificacao, Ax = \", A*x)\n",
    "println(\"lado direito    = \", b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um fato que pode ser importante em alguns casos é que uma vez calculada a fatoração LU de uma matriz ela pode ser utilizada sempre no lugar de $A$. Assim, por exemplo, se quisermos resolver sistemas lineares com diferentes lados direitos (diferentes vetores $b$) a fatoração pode ser feita uma única vez.\n",
    "\n",
    "## Tempo de execução (complexidade)\n",
    "\n",
    "Ao escrevermos programas de computador é importante que tenhamos uma ideia do seu tempo de execução. Muitas vezes não basta inventar um algoritmo que esteja correto, é preciso que ele seja eficiente. Isso é um pouco menos importante nos dias de hoje do que era há algumas décadas, já que os computadores estão muito mais poderosos. Mas ainda é fácil escrever programas que demoram demais para realizar as suas tarefas.\n",
    "\n",
    "Quando falamos em programas numéricos, cujo principal objetivo é realizar longas sequencias de cálculos, podemos buscar adquirir uma ideia do tempo de execução, ou complexidade de um algoritmo, através da contagem do número de operações de ponto flutuantes realizadas. Isso é apenas uma aproximação da complexidade total do processo, já que outras operações que não envolvem cálculos com números reais não são contadas. Isso pode ser particularmente dramático por desprezar a movimentação de dados entre a memória e o processador, algo que é muito demorado em computadores modernos. A contagem do número de operações continua sendo usada, porém, porque geralmente uma implementação cuidadosa de um método numérico é tipicamente limitada por essas operações. \n",
    "\n",
    "Usaremos a sigla FLOP (de *Foating Point OPeration*) para designar uma operação básica de ponto flutuante como a soma, subtração, multiplicação, divisão ou extração de raiz quadrada. Nosso objetivo nessa seção é contar a quantidade de FLOPs de cada um dos principais algoritmos implementados até agora.\n",
    "\n",
    "### Substituição\n",
    "\n",
    "Vamos começar pela `subs_reg`. Olhando o seu código, vemos que a maior parte do trabalho ocorre dentro do laço `for`. Para calcular o novo `x[i]` precisamos primeiro atualizar o lado direito no laço `for j`. Esse laço executa $n - (i + 1) + 1 = n - i$ produtos e subtrações. Totalizand $2(n - i)$ FLOPs. Depois basta fazer mais uma divisão. Deste modo, o número total de FLOPs em cada execução do laço externo, `for i` é $2(n - i)+ 1$. Podemos, finalmente, calcular o trabalho total:\n",
    "$$\n",
    "\\sum_{i = 1}^{n} 2(n - i) + 1 =  \\sum_{i = 1}^{n} (2n + 1) - 2 \\sum_{i = 1}^{n} i = n(2n + 1) - n(n + 1) = n^2.\n",
    "$$\n",
    "Obs: vejam que escrevi a somatória com i indo de 1 até n, no lugar de cair de n até 1. Isso é possível porque a ordem das parcelas não altera a soma.\n",
    "\n",
    "Vejam que o tempo é bem razoável, afinal uma matriz triangular tem cerca de $n^2 / 2$ entradas não nulas, então fazemos cerca de duas operações de ponto flutuante (FLOPs) por entrada da matriz. É difícil imaginar algo muito melhor do que isso.\n",
    "\n",
    "Por fim, como o algoritmo de `subs_prog` (progressiva) é apenas uma versão invertida da `subs_reg`, o número de operações será o mesmo. Faça os detalhes como exercício.\n",
    "\n",
    "### Fatoração LU\n",
    "\n",
    "Agora vamos para algo mais desafiador. Vamos tentar avaliar o tempo gasto na rotina `preLU`. Ela tem dois laços. Vamos primeiro analisar o número de operações no laço interno, o `for j`. Ele roda $n - (i + 1) + 1 = n - i$ vezes. Em cada uma de suas passagens ele executa uma divisão, para calcular `coef`, seguida de $n - (i + 1) + 1 = n - i$ produtos e somas. Totalizando $2(n - i) + 1$ FLOPs. Multiplicando isso pelo número de vezes que esse laço interno roda, temos que no total o laço inteno usa\n",
    "$$\n",
    "2(n - i)^2 + (n - i)\n",
    "$$\n",
    "FLOPs. Note que as duas linhas do meio desse laço internonão geram FLOPs. Nelas, são realizadas apenas cópias de valores em memória. Estamos prontos para escrever a somatória que irá acumular todo o trabalho feito.\n",
    "$$\n",
    "\\sum_{i = 1}^{n - 1} 2(n - i)^2 + (n - i) = [2(n - 1)^2 + (n - 1)] + [2(n - 2)^2 + (n - 2)] + \\ldots + [2\\cdot2^2 + 2] + [2\\cdot1^2 + 1]. \n",
    "$$\n",
    "Note que lendo essa somatória de trás para frente chegamos a uma expressão um pouco mais simples.\n",
    "$$\n",
    "\\sum_{i = 1}^{n - 1} 2i^2 + i.\n",
    "$$\n",
    "\n",
    "Aqui nos deparamos com uma dificuldade. Como calcular a soma $\\sum_{i = 1}^n i^2$? Isso tem uma resposta conhecida que é\n",
    "$$\n",
    "\\sum_{i = 1}^n i^2 = \\frac{n (n + 1) (2n + 1)}{6}.\n",
    "$$\n",
    "Você pode encontrar várias demonstrações disso. Um exemplo legal é [essa](http://blog.jgc.org/2008/01/proof-that-sum-of-squares-of-first-n.html). Mas há várias outras demonstrações.\n",
    "\n",
    "Retornando ao nosso problema queremos\n",
    "\\begin{align*}\n",
    "    \\sum_{i = 1}^{n - 1} 2i^2 + i & = \\frac{(n-1)n(2n - 1)}{3} + \\frac{(n-1)n}{2} \\\\\n",
    "    &= (n-1)n \\left(\\frac{2n - 1}{3} + \\frac{1}{2} \\right) \\\\\n",
    "    &= (n - 1)n \\left( \\frac{2n}{3} + \\frac{1}{6} \\right) \\\\\n",
    "    &= \\frac{2n^3}{3} - \\frac{n^2}{2} - \\frac{n}{6} \\\\\n",
    "    &\\approx \\frac{2n^3}{3}.\n",
    "\\end{align*}\n",
    "A aproximação final vale para $n$ grande, que é o que interessa quando o problema aumenta.\n",
    "\n",
    "De fato, em geral, nós estamos interessados apenas nos termos de maior grau, assim podemos simplificar muito as contas eliminando termos intermediários que não serão importantes (para $n$ grande). As contas acima ficariam com a cara\n",
    "\\begin{align*}\n",
    "    \\sum_{i = 1}^{n - 1} 2i^2 + i &\\approx \\sum_{i = 1}^{n - 1} 2i^2 \\\\\n",
    "    &= 2 \\frac{(n-1)n(2n - 1)}{6} \\\\\n",
    "    &\\approx \\frac{2n^3}{3}.\n",
    "\\end{align*}\n",
    "\n",
    "Note que o tempo de calcular uma fatoração LU é muito maior do que o tempo de resolver um sistema triangular por substituição, afinal ele cresce com o cubo da dimensão da matriz. Isso mostra que, para resolver um sistema linear fazendo primeiro a fatoração LU, a maior parte do trabalho está no processo de fatoração. Ele irá dominar o tempo de execução.\n",
    "\n",
    "## Pivoteamento\n",
    "\n",
    "O processo de fatoração LU descrito parece, de fato, muito interessante. Uma pergunta natural é se essa fatoração existe para qualquer matriz quadrada. A resposta para isso pode ser retirada do próprio processo de fatoração. Olhando o algoritmo vemos que o algoritmo consegue ir até o fim se, e somente se, a cada iteração o termo $a_{ii}$ usando para calcular os coeficientes for não nulo. Ou seja o algoritmo irá falhar se, e somente se, após $k - 1$ iterações encontrarmos a seguinte situação:\n",
    "$$\n",
    "\\tilde U = \\left(\n",
    "\\begin{array}{cccc|c|ccccc}\n",
    "X      &X      &X      &X      &\\dots  &X      &X      &\\dots &X \\\\\n",
    "0      &X      &X      &X      &\\dots  &X      &X      &\\dots &X \\\\\n",
    "0      &0      &X      &X      &\\dots  &X      &X      &\\dots &X \\\\\n",
    "\\vdots &\\vdots &\\ddots &\\ddots &\\dots  &\\vdots &\\vdots &\\dots &X \\\\ \\hline\n",
    "0      &0      &0      &\\dots  &0      &X      &X      &\\dots &X \\\\ \\hline\n",
    "\\vdots &\\vdots &\\vdots &\\vdots &X      &X      &X      &\\dots &X \\\\\n",
    "0      &0      &0      &0      &X      &X      &X      &\\dots &X\n",
    "\\end{array}\n",
    "\\right).\n",
    "$$\n",
    "Acima, os elementos potencialmente não nulos estão denotados por $X$ e a linha e coluna $k$ estão destacadas. \n",
    "\n",
    "Agora na $k$-ésima iteração o objetivo é zerar os elementos abaixo da posição $k \\times k$ usando o elemento da diagonal que infelizmente é $0$. O algoritmo não pode continuar. Em particular a porção superior esquerda com $k$ linhas e $k$ colunas dessa matriz, que vamos denotar por $U_k$, foi obtida através de um processo de escalonamento da mesma porção da matriz original, isso é de $A[1:k, 1:k]$. Relembrando a discussão que nos levou a descobrir a ideia da fatoração LU isso nos ensina que existem matrizes $L_1, \\ldots, L_{k -1} \\in \\mathbb{R}^{k \\times k}$ inversíveis tais que\n",
    "$$\n",
    "L_{k-1} \\dots L_1 A[1:k, 1: k] = \\tilde{U}[1:k, 1:k].\n",
    "$$\n",
    "Como as matrizes $L_{i},\\ i = 1, \\dots, k-1$ são inversíveis e a matriz $\\tilde{U}[1:k, 1:k]$ não é inversível, pois tem a última linha de zeros, vemos que a matriz $A[1:k, 1:k]$ também não é inversível. Isso pode ser confirmado de muitas formas. Por exemplo, lembrando que um sistema baseado em $A[1:k, 1:k]$ e em $\\tilde{U}[1:k, 1:k]$ tem que ter as mesmas soluções, com possíveis adaptações do lado direito. Claramente um sistema baseado em $\\tilde{U}[1:k, 1:k]$ pode ter nenhuma ou infinitas soluções, mas nunca solução única. Isso caracteriza uma matriz não inversível. Outra forma ainda mais fácil é usar a propriedade que o determinante de um produto de matrizes é igual ao produto dos determinantes e lembrar do fato que uma matriz é inversível se, e somente, se seu determinante for não nulo. Isso nos leva ao seguinte resultado:\n",
    "\n",
    "**Teorema.** Uma matriz $A \\in \\mathbb{R}^{n \\times n}$ pode ser fatorada pelo algoritmo `preLU`, que chamaremos de *fatoração LU sem pivoteamento*, se, e somente se, as submatrizes $A[1:k, 1:k],\\ k = 1 \\ldots n - 1$, tiverem determinantes não nulos.\n",
    "\n",
    "Obs: Os determinantes das submatrizes $A[1:k, 1:k],\\ k = 1 \\ldots n$ são conhecidos como *menores principais* de $A$.\n",
    "\n",
    "Mas o que fazer caso algum menor pricipal tiver determinante nulo? Olhando de novo para a matriz acima, podemos ter a ideia de tentar procurar na coluna $k$ abaixo da diagonal um elemento não nulo de $A$. Se isso for possível, trocamos a linha $k$ pela linha desse elemento (que seria equivalente a trocar duas equações de posição no sistema linear) e obtemos um sistema equivalente para o qual o algoritmo pode continuar. Essa operação de troca é de novo uma operação elementar, que pode ser representada por uma matriz identidade com as respectivas linhas trocadas, multiplicada à esquerda. Verifique.\n",
    "\n",
    "A aplicação dessa regra permite calcular uma fatoração LU, não mais de $A$ mas de uma versão de $A$ com linhas permutadas. Esse tipo de matriz pode ser representado pela multiplicação à esquerda de $A$ por uma matriz de permutação $P$ que é uma matriz identidade com linhas trocadas de lugar representando as várias trocas de linhas necessárias para levar o processo à cabo. Ou seja, ao final obteremos algo na forma\n",
    "$$\n",
    "PA = LU.\n",
    "$$\n",
    "\n",
    "Mais uma vez vemos que agora o método só pára se não encontrar nenhum elemento não nulo da posição $k \\times k$ para baixo, ou seja se a matriz parcial tiver a forma\n",
    "$$\n",
    "\\tilde{U} = \\left(\n",
    "\\begin{array}{cccc|c|cccc}\n",
    "X      &X      &X      &X      &\\dots   &X      &X      &\\dots &X \\\\\n",
    "0      &X      &X      &X      &\\dots  &X      &X      &\\dots &X \\\\\n",
    "0      &0      &X      &X      &\\dots  &X      &X      &\\dots &X \\\\\n",
    "\\vdots &\\vdots &\\ddots &\\ddots &\\dots  &\\vdots &\\vdots &\\dots &X \\\\ \\hline\n",
    "0      &0      &0      &\\dots  &0      &X      &X      &\\dots &X \\\\ \\hline\n",
    "\\vdots &\\vdots &\\vdots &\\vdots &\\vdots &X      &X      &\\dots &X \\\\\n",
    "0      &0      &0      &0      &0      &X      &X      &\\dots &X\n",
    "\\end{array}\n",
    "\\right).\n",
    "$$\n",
    "Mas essa matriz tem determinante nulo. Para ver isso, comece fazendo a expansão do seu determinante pela primeira coluna. Como a coluna é quase inteira de zero vemos que esse determinante é o $\\tilde{u}_{11} \\det(\\tilde{U}[2:n, 2:n])$. Continuando esse processo até a coluna $k$ obtemos \n",
    "$$\n",
    "\\det(\\tilde{U}) = \\tilde{u}_{11} \\tilde{u}_{22} \\ldots \\tilde{u}_{kk} \\det(\\tilde{u}[k+1:n, k+1:n]) = 0.\n",
    "$$\n",
    "A última igualdade vem do fato que $u_{kk} = 0$.\n",
    "\n",
    "Concluímos que o processo de fatoração LU com pivoteamento parcial somente pára se a matriz $\\tilde{U}$ encontrada for não inversível. Mais uma vez essa matriz é obtida a partir de $A$ pelo produto de matrizes triangulares inversíveis e portanto concluímos que o processo somente pára se $A$ é não inversível. Provamos um novo resultado.\n",
    "\n",
    "**Teorema** Se uma matriz $A \\in \\mathbb{R}^{n \\times n}$ for inversível, então ela admite fatoração LU com pivoteamento parcial.\n",
    "\n",
    "Nosso próximo desafio é adaptar a rotina `preLU` para incorporar o pivoteamento parcial. Para isso é necessário guardar a informação de onde as linhas foram parar, e trocar linhas de $U$ e $L$ quando o pivoteamento for feito. Veja abaixo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fatoracao LU de uma matriz A com pivoteamento parcial\n",
    "function LU(A)\n",
    "    # Reserva espaco para a resposta\n",
    "    n, _ = size(A)\n",
    "    # Cria um vetor para guardar a posição das linhas\n",
    "    P = collect(1:n)\n",
    "    L = one(A)\n",
    "    U = copy(A)\n",
    "    # Coloca zeros abaixo da posicao i x i trazendo para linha i a com \n",
    "    # maior pivot.\n",
    "    for i = 1:n - 1\n",
    "        # Busca o maior pivot em valor absoluto\n",
    "        maxind = argmax(abs.(U[i:end, i])) + i - 1\n",
    "        # Troca as linhas de lugar e guarda a informação.\n",
    "        U[i, i:end], U[maxind, i:end] = U[maxind, i:end], U[i, i:end]\n",
    "        L[i, 1:i-1], L[maxind, 1:i-1] = L[maxind, 1:i-1], L[i, 1:i-1]\n",
    "        P[i], P[maxind] = P[maxind], P[i]\n",
    "        # Continua com a fatoração LU.\n",
    "        for j = i + 1:n\n",
    "            coef = U[j, i] / U[i, i]\n",
    "            L[j, i] = coef\n",
    "            U[j, i] = 0.0\n",
    "            U[j, i + 1:end] .-= coef .* U[i, i + 1:end]\n",
    "        end\n",
    "    end\n",
    "    return P, L, U\n",
    "end\n",
    "\n",
    "A = [2.0 3 1 1; 4 7 4 3; 4 7 6 4; 6 9 9 8]\n",
    "#A = [1.0 2 1 1; 2 3 1 1; 2 4 5 1; 1 1 1 1]\n",
    "b = [3.0, 6, 4, 3]\n",
    "P, L, U = LU(A);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L*U"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note que a matriz $LU$ obtida ao final é igual à matriz $A$ com a primeira linha trocada com a quarta, como está indicado no vetor que guardas as permutações `P`. \n",
    "\n",
    "Como podemos usar isso para resolver um sistema $Ax = b$? Como a matriz $LU$ ao final representa o mesmo sistema com algumas equações com ordem trocada, basta aplicar a mesma troca de ordem para componentes de $b$ e resolver o sistema usando $LU$. Vejamos a implementação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resolve sistema Ax = b por LU com pivoteamento parcial.\n",
    "function resolve_LU(A, b)\n",
    "    P, L, U = LU(A)\n",
    "    # Troca as coordenadas de b de lugar.\n",
    "    b = b[P]\n",
    "    y = subs_prog(L, b)\n",
    "    x = subs_reg(U, y)\n",
    "    return x\n",
    "end\n",
    "\n",
    "# Por fim, usamos as rotinas implementadas até agora para resolver um sistema linea.\n",
    "A = [2.0 3 1 1; 4 7 4 3; 4 7 6 4; 6 9 9 8]\n",
    "b = [3.0, 6, 4, 3]\n",
    "x = resolve_LU(A, b)\n",
    "println(\"Solucao         = \", x)\n",
    "println(\"Verificacao, Ax = \", A*x)\n",
    "println(\"Lado direito    = \", b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Pivoteamento completo\n",
    "\n",
    "Uma alternativa ao pivoteamento parcial é o pivoteamento completo. Nessa variação, no início da iteração $k$ o algoritmo não procura trazer para a posição $(k, k)$ da diagonal o maior elemento em módulo apenas do final da coluna $k$ de $\\tilde{U}$ que está abaixo da diagonal. Podemos querer ser mais gulosos e procurar o maior elemento em toda a porção inferior direita de $\\tilde{U}$, ou seja em $\\tilde{U}[k:n, k:n]$. Imagine que esse elemento está na posição $(i, j)$. Podemos trazer esse elemento para a posição $(k, k)$ trocando a linha $i$ com a $k$ de lugar e depois as colunas $j$ e $k$. Esse método pode continuar sempre, a menos que toda a matriz $\\tilde{U}[k:n, k:n]$ seja composta por zeros, ou seja o algoritmo só para se ele se deparar com a configuração\n",
    "$$\n",
    "\\tilde{U} = \\left(\n",
    "\\begin{array}{cccc|c|cccc}\n",
    "X      &X      &X      &X      &\\dots  &X      &X      &\\dots &X \\\\\n",
    "0      &X      &X      &X      &\\dots  &X      &X      &\\dots &X \\\\\n",
    "0      &0      &X      &X      &\\dots  &X      &X      &\\dots &X \\\\\n",
    "\\vdots &\\vdots &\\ddots &\\ddots &\\dots  &\\vdots &\\vdots &\\dots &X \\\\ \\hline\n",
    "0      &0      &0      &\\dots  &0      &0      &0      &\\dots &0 \\\\ \\hline\n",
    "\\vdots &\\vdots &\\vdots &\\vdots &\\vdots &0      &0      &\\dots &0 \\\\\n",
    "0      &0      &0      &0      &0      &0      &0      &\\dots &0\n",
    "\\end{array}\n",
    "\\right).\n",
    "$$\n",
    "Mas nesse caso a matriz $\\tilde{U}$ já é triangular superior! O método pode parar pois não há mais o que fazer. Observe ainda que já sabíamos que o processo de troca de duas linhas pode ser representado pela multiplicação à esquerda de uma matriz permutação obtida da identidade trocando-se duas de suas linhas. Já a troca de colunas pode ser representada por uma multiplicação por uma matriz de permutação à direita. Isso pode ser explorado para provar o seguinte teorema:\n",
    "\n",
    "**Teorema** Toda matriz quadrada $A \\in \\mathbb{R}^{n \\times n}$ admite uma fatoração LU com pivoteamento completo. Ou seja, existem matrizes de permutação $P$ e $Q$, obtidas trocando-se linhas e colunas da matriz identidade de lugar, uma matriz triangular inferior $L$ e uma matriz triangular superior $U$ tal que\n",
    "$$\n",
    "P A Q = LU.\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "# Erros numéricos e número de condição\n",
    "\n",
    "Bom, conseguimos então escrever uma rotina que é capaz de calcular a fatoração PLU de qualquer matriz inversível dada. Ela pode então ser usada para resolver sistemas. Porém, a necessidade de pivoteamento foi introduzida apenas em um caso muito radical, que é quando o elemento da diagonal encontrado é 0. Já a implementação faz algo ainda mais robusto. Ela sempre traz para a diagonal o maior elemento em módulo. Será que isso era realmente necessário? Ou será que estamos fazendo trabalho à toa e deveríamos mover elementos apenas no caso de encontrar zero?\n",
    "\n",
    "Vamos pensar um pouco analisando o seguinte caso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = [1.0e-20 1; 1 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L, U = preLU(A)\n",
    "L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L*U"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notem que a matriz $LU$ obtida é muito diferente de $A$, isso pode ser visto claramente comparando os elementos $(2, 2)$. Como entender o que aconteceu? Nesse caso a única coisa que o algoritmo `preLU` faz é alterar a última linha para\n",
    "$$\n",
    "[1.0,\\quad 1.0] - 10^{20} [ 10^{-20},\\quad 1].\n",
    "$$\n",
    "Se as contas tivessem sido realizadas exatamente teríamos ao final\n",
    "$$\n",
    "[0,\\quad 1-10^{20}].\n",
    "$$\n",
    "Porém as contas não são realizadas exatamente, elas são realizadas em ponto flutuante no computador e sabemos que a precisão é de cerca de 16 casas decimais ($\\epsilon_{mac} \\approx 10^{-16}$). Nesse caso o $1$ é tão menor em módulo que o $-10^{20}$ que ele acaba sendo ignorado. Mas depois ele faz falta ao se tentar remontar a matriz $A$ a partir dos seus fatores. A situação também se mostra muito ruim se tentamos usar os fatores calculados para resolver um sistema\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = [1.0, 0]\n",
    "y = substituicao(L, b)\n",
    "x = retro_substituicao(U, y)\n",
    "println(\"b = \", b)\n",
    "println(\"Solucao = \", x)\n",
    "println(\"Verificacao, Ax = \", A*x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note que o valor do $Ax$ obtido, aproximadamente $(1, 1)'$, é muito diferente do lado direito original $(1, 0)'$. Isso pode sempre ocorrer quando o valor o pivot for muito pequeno em módulo, já que ele entra dividindo a linha da matriz que será somada às outras. Nesse caso a linha dividida pelo pivot pode ficar muito grande e ao usar esses coeficientes para gerar a linha que será somada às outras essa linha fica tão grande que tende a apagar a informação das demais. Isso mostra que não devemos apenas trocar linhas quando o valor na diagonal for $0$, queremos também evitar valores pequenos. Este é o motivo de porque é desejável sempre trocar as linhas de forma a trazer o maior valor possível para a posição do pivot. De fato veja o que obteríamos se usássemos a rotina com pivoteamento parcial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P, L, U = LU(A)\n",
    "L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L*U"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veja que a matriz $LU$ obtida recupera a matriz $A$ original com as linhas trocadas como esperado. Além disso se resolvermos o sistema teremos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = resolve_LU(A, b)\n",
    "println(\"b = \", b)\n",
    "println(\"Solucao = \", x)\n",
    "println(\"Verificacao, Ax = \", A*x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veja que agora o valor correto do lado direito correto é recuperado.\n",
    "\n",
    "### Número de condição\n",
    "\n",
    "Vimos anteriormente que a menos que tomemos cuidado de fazer pivoteamento parcial a fatoração LU pode falhar em resolver sistemas lineares simples. Pelo menos a precisão final obtida pode ser muito ruim.\n",
    "\n",
    "Vamos agora ver que há sistemas lineares que tem uma propriedade intrínseca que impede que eles sejam resolvidos com alta precisão. Para entender isso vamos apresentar dois casos de sistemas no plano e relembrar um pouco sobre a sua solução geométrica.\n",
    "\n",
    "Dado um sistema de duas equações e duas icógnitas, por exemplo\n",
    "\\begin{align*}\n",
    "x + y &= 2 \\\\\n",
    "x - 2y &= -1.\n",
    "\\end{align*}\n",
    "Aprendemos que podemos resolvê-lo graficamente desenhando as retas que representam cada uma das duas equações e procurando o seu ponto de cruzamento. Nesse caso teríamos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using PyPlot\n",
    "x = linspace(0.0, 2.0, 100)\n",
    "# Para obter as funções abaixo basta resolver as equações para y.\n",
    "plot(x, 2 - x)\n",
    "plot(x, (x + 1)/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nessa imagem vemos claramento o ponto de cruzamento que é $(1, 1)$. A situação ficaria bem menos clara se as duas retas fossem quase paralelas. Isso ocorre por exemplo com o sistema\n",
    "\\begin{align*}\n",
    "x + y &= 2 \\\\\n",
    "(1.0 + 10^{-1})x + y &= 2 + 10^{-1}.\n",
    "\\end{align*}\n",
    "\n",
    "Nesse caso a figura fica:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(x, 2 - x)\n",
    "plot(x, 2.1 - 1.1x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora o ponto de intersecção continua sendo $(1, 1)$ mas isso é muito menos claro visualmente. A única solução é ir tentando aumentar a imagem próximo à região de intersecção (zoom) para tentar ver melhor. Por outro lado se você imaginar que as linhas tem espessura fixa o zoom não vai adiantar muito. Há uma precisão máxima que pode ser obtida. De forma análoga ao caso de linhas de espessura fixa, a precisão que pode ser atingida com números do tipo ponto flutuante é também limitada.\n",
    "\n",
    "Dessa forma, é de se esperar que quando queremos resolver um sistema associado a equações quase paralelas o computador tenha problemas. Vamos ver isso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constroi um sistema que tem solucao exata (1, 1) mas com a segunda \n",
    "# equacao muito parecida com a primeira.\n",
    "pertubacao = 1.0e-8\n",
    "A = [1.0 1.0; 1.0 + pertubacao 1.0 - pertubacao]\n",
    "b = [2.0 2.0]\n",
    "x = resolve_LU(A, b)\n",
    "println(\"Solucao calculada = \", x[1], \" \", x[2])\n",
    "println(\"Erro relativo = \", norm(x - ones(2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veja que a solução calculada já erra na nona casa. Isso é confirmado pelo calculo do erro relativo. Um fato interessante é que é possível calcular um valor a partir da matriz que nos diz quando esperar que problemas numéricos como os que vimos acima podem ocorrer. Mas para isso precisamos fazer um pequeno desvio e falar sobre normas de matrizes.\n",
    "\n",
    "### Distâncias e normas de vetores e matrizes.\n",
    "\n",
    "Em Matemática chamamos de norma, ou comprimento de um vetor $x \\in \\mathbb{R}^n$, uma função $x \\mapsto \\| x \\| \\in \\mathbb{R}$ tal que:\n",
    "\n",
    "1. $\\| x \\| \\geq 0$ para todo $x \\in \\mathbb{R}^n$ e $\\| x \\| = 0$ somente se $x = 0$.\n",
    "\n",
    "1. $\\| \\alpha x \\| = |\\alpha| \\| x \\|$, para todo $x \\in \\mathbb{R}^n$ e $\\alpha \\in \\mathbb{R}$.\n",
    "\n",
    "1. $\\| x + y \\| \\leq \\| x \\| + \\| y \\|$ para todos $x, y \\in \\mathbb{R}^n$ (desigualdade triangular).\n",
    "\n",
    "A ideia por detrás dessa definição é capturar diferentes formas de medir tamanho ou distância, mas preservando a propriedade que vetores pequenos tem tamanhos pequenos, somente o vetor nulo tem tamanho $0$ e que o tamanho do lado de um triângulo é menor ou igual a soma dos tamanhos dos outros dois lados.\n",
    "\n",
    "Com a norma definida podemos definir a distância entre dois vetores como a norma de sua diferença, como é natural.\n",
    "\n",
    "Vejamos agora alguns os exemplos mais importantes de norma nesse curso.\n",
    "\n",
    "\\begin{align*}\n",
    "\\| x \\|_1 &= | x_1 | + | x_2 | + \\ldots + | x_n |. \\\\\n",
    "\\| x \\|_2 &= \\sqrt{x_1^2 + x_2^2 + \\ldots + x_n^2}. \\\\\n",
    "\\| x \\|_\\infty &= \\max \\{ | x_1 |, | x_2 |, \\ldots, | x_n | \\}.\n",
    "\\end{align*}\n",
    "\n",
    "É fácil mostrar que as definições acima obedecem às três propriedades que definem uma norma usando as propriedades relacionadas da função módulo.\n",
    "\n",
    "Uma pergunta natural é: porque se preocupar com formas alternativas de medir comprimento de vetor ou distância entre dois vetores que não seja a distância usual, euclidiana, que é capturada na definição da norma 2? Para entender disso imagine que você mora em uma cidade com ruas formando um quadriculado. Se você está em um ponto $(x_1, y_1)$ nessa cidade e quer ir para o ponto $(x_2, y_2)$ é fácil entender que o mínimo que você precisa se movimentar é justamente $| x_1 - x_ 2 |$ na horizontal $| y_1 - y_2 |$ na vertical. Ou seja, na prática a distância entre esses pontos é $| x_1 - x_ 2 | + | y_1 - y_2 |$ que está justamente relacionada à norma 1.\n",
    "\n",
    "Da mesma forma que podemos estar interessados em definir o comprimento ou de vetores podemos também querer definir o comprimento ou tamanho de matrizes. Isso é usualmente feito referindo-se a normas de vetores. Vamos mais uma vez apresentar as definições mais importantes para o curso. Seja $A \\in \\mathbb{R}^{n \\times n}$ temos\n",
    "\\begin{align*}\n",
    "\\| A \\|_1 &= \\max \\{ \\| a_{1:n, i} \\|_1,\\ i = 1, \\ldots, n \\} \\quad\\quad \\text{(máxima norma 1 das colunas de A)}. \\\\\n",
    "\\| A \\|_2 &= \\max_{\\| x \\|_2 = 1} \\{ \\| Ax \\|_2 \\}.\\\\\n",
    "\\| A \\|_\\infty &= \\max \\{ \\| a_{i, 1:n} \\|_1,\\ i = 1, \\ldots, n \\} \\quad\\quad \\text{(máxima norma 1 das linhas de A)}. \\\\\n",
    "\\end{align*}\n",
    "\n",
    "Uma propriedade fundamental que relaciona as normas de matrizes com as respectivas normas de vetores é apresentada a seguir.\n",
    "\n",
    "**Proposição.** \n",
    "$$\n",
    "\\| Ax \\|_\\dagger \\leq \\| A \\|_\\dagger \\| x \\|_\\dagger,\n",
    "$$\n",
    "em que $\\dagger$  pode ser substituido (nas três posições ao mesmo tempo) por $1$, $2$, ou $\\infty$.\n",
    "\n",
    "Vejamos um exemplo numérico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = reshape(1:25, 5, 5)\n",
    "@show norm(A, 1)\n",
    "@show norm(A, 2)\n",
    "\n",
    "@show norm(A, Inf);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De posse dessas definições, estamos prontos para retomar o nosso objetivo original: apresentar um valor que pode ser calculado a partir da matriz associada ao sistema que desejamos resolver e que seja capaz de estimar a risco de corremos de ter grandes erros numéricos.\n",
    "\n",
    "### Número de condição\n",
    "\n",
    "Esse valor é conhecido como *número de condição* e pode ser entendido como uma forma de estimar o quanto a matriz está perto de ser não-inversível, ou seja o quão perto suas linhas (ou colunas) estão de se tornarem linearmente dependentes. Ele é dado por\n",
    "$$\n",
    "\\kappa(A) = \\| A \\| \\| A^{-1} \\|.\n",
    "$$\n",
    "\n",
    "Vamos inicialmente calcular o número de condição das duas matrizes associados aos sistemas no plano vistos anteriormente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = [1 1; 1 -2]\n",
    "norm(A)*norm(inv(A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = [1.0 1.0; 1.0 + pertubacao 1.0 - pertubacao]\n",
    "norm(A)*norm(inv(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como vocês podem ver o número de condição da segunda matriz é muito grande, da ordem de $10^8$. Isso sugere que é possível que encontremos dificuldades numéricos ao calcular a fatoração LU (mesmo usando pivoteamento) dessa matriz ou ao tentar resolver um sistema linear baseado nela, como já observamos.\n",
    "\n",
    "### Continuando com número de condição (leitura opcional)\n",
    "\n",
    "*Falta escrever*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Métodos iterativos\n",
    "\n",
    "Os metodos que apresentamos acima, escalonamento (eliminação de Gauss) e fatoração LU, têm como característica a modificação do sistema original para colocá-lo em um formato que pode ser resolvido rapidamente. O grosso do trabalho, que é da ordem $O(\\frac{2}{3} n^3)$, é gasto nesse processo de transformação do sistema ou da respectiva matriz. Até que ele seja terminado não se obtém nenhuma aproximação da solução. Esse tipo de método tem então uma característica do tipo \"tudo ou nada\". Ou o usuário espera que todo o trabalho seja feito ou ele sai sem nenhuma resposta. Esses métodos são conhecidos como métodos diretos.\n",
    "\n",
    "Uma alternativa ao métodos diretos são os métodos iterativos. Neles o objetivo é aproximar, o mais rapidamente possível, a resposta. O que se perde é que em geral não é possível calculá-la exatamente. Esses métodos são usados quando, por exemplo, $n$ é muito grande e então não é possível esperar que um método direto termine o seu trabalho. Outra situação onde pode ser desejável usar métodos iterativos é quando a matriz $A$ possui muitos elementos nulos. Nesse caso dizemos que $A$ é uma matriz *esparsa*. Voltaremos a comentar isso depois.\n",
    "\n",
    "Vamos apresentar agora as ideias por trás do método iterativo mais simples. Ele se baseia em uma observação trivial. Para fixar as ideias vamos iniciar com um sistema $2$ por $2$.\n",
    "$$\n",
    "\\left\\{ \\begin{array}{rcrcl}\n",
    "2.5x &-& y &=& 1.5 \\\\\n",
    "x &-& 2y &=& -1.\n",
    "\\end{array}\\right.\n",
    "$$\n",
    "Podemos facilmente resolver o sistema e verificarque é solução é em $(x, y) = (1, 1)$. Veja o gráfico dele abaixo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using PyPlot\n",
    "\n",
    "x = linspace(0.8, 2.0, 100)\n",
    "eq1(x) = 2.5*x - 1.5\n",
    "eq2(x) = 0.5*(1.0 + x)\n",
    "plot(x, eq1(x), color=\"blue\")\n",
    "plot(x, eq2(x), color=\"red\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma outra forma de encarar as equações do sistema é vê-las como fórmulas de como calcular uma das variáveis das soluções se conhecêssemos a outra. Isso fica claro se reorganizarmos um pouco as duas equações.\n",
    "\\begin{align*}\n",
    "x &= (1.5 + y)/2.5 \\\\\n",
    "y &= (1 + x)/2.\n",
    "\\end{align*}\n",
    "A ideia do método de Jacobi é melhorar uma aproximação da solução que obtivemos até o momento $k$, que vamos denotar por $(x^k, y^k)$, usando essas duas equações como se elas estivessem partindo da solução exata. Isto é fazer\n",
    "\\begin{align*}\n",
    "x^{k+1} &= (1.5 + y^k)/2.5 \\\\\n",
    "y^{k+1} &= (1 + x^k)/2.\n",
    "\\end{align*}\n",
    "\n",
    "Geometricamente, o que a primeira equação faz é encontrar a coordenada $x$ do ponto na reta azul que tem coordenada $y = y^k$, ou seja a intersecção da reta azul com uma reta paralela ao eixo x que passar por $(x^k, y^k)$. Já a segunda equação busca a coordenada $y$ do ponto na reta vermelha que cruza com uma reta vertical que passa também por $(x^k, y^k)$.\n",
    "\n",
    "Para entender isso melhor, imagine que temos $(x^k, y^k) = (1.4, 1.3)$. O código abaixo representa as contas feitas a apresenta o novo ponto calculado bem como o ponto de partida. Note que o novo ponto se aproxima da solução que é a intersecção das retas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passo de Jacobi\n",
    "# Apresenta o grafico inicial\n",
    "plot(x, eq1(x), color=\"blue\")\n",
    "plot(x, eq2(x), color=\"red\")\n",
    "\n",
    "# Apresenta o ponto de partida e as linhas que geram o novo ponto\n",
    "xk, yk = 1.4, 1.3\n",
    "unsx = ones(length(x))\n",
    "y = 0.5:0.01:3.0\n",
    "unsy = ones(length(y))\n",
    "plot(xk, yk, marker=\"o\", color=\"black\")\n",
    "\n",
    "# Calcula o novo ponto e o apresenta\n",
    "xk1 = (1.5+ yk)/2.5\n",
    "yk1 = (1.0 + xk)/2\n",
    "deltax = xk1:0.01:xk\n",
    "deltay = yk1:0.01:yk\n",
    "plot(deltax, yk*ones(length(deltax)), color=\"black\", linestyle=\"--\")\n",
    "plot(xk*ones(length(deltay)), deltay, color=\"black\", linestyle=\"--\")\n",
    "plot(xk1, yk1, marker=\"o\", color=\"black\")\n",
    "plot(deltax, yk1*ones(length(deltax)), color=\"black\", linestyle=\"--\")\n",
    "plot(xk1*ones(length(deltay)), deltay, color=\"black\", linestyle=\"--\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ainda continuar fazendo isso, gerando então uma sequencia e ver se ela se aproxima ou não da solução:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Método de Jacobi\n",
    "# Altere esses parametros para ver o novo comportamento.\n",
    "# Ponto incial\n",
    "xk, yk = 1.8, 2.5\n",
    "# Numero de iteracoes\n",
    "maxiter = 5\n",
    "\n",
    "# Apresenta o grafico inicial\n",
    "plot(x, eq1(x), color=\"blue\")\n",
    "plot(x, eq2(x), color=\"red\")\n",
    "\n",
    "# Apresenta o ponto de partida em verde e as linhas que geram o novo ponto\n",
    "unsx = ones(length(x))\n",
    "y = 0.5:0.01:3.0\n",
    "unsy = ones(length(y))\n",
    "plot(xk, yk, marker=\"o\", color=\"green\")\n",
    "\n",
    "# Calcula os novos pontos e os apresenta\n",
    "for i = 1:maxiter\n",
    "    xk1 = (1.5 + yk)/2.5\n",
    "    yk1 = (1.0 + xk)/2.0\n",
    "    deltax = xk1:0.01:xk\n",
    "    deltay = yk1:0.01:yk\n",
    "    plot(deltax, yk*ones(length(deltax)), color=\"black\", linestyle=\"--\")\n",
    "    plot(xk*ones(length(deltay)), deltay, color=\"black\", linestyle=\"--\")\n",
    "    plot(xk1, yk1, marker=\"o\", color=\"black\")\n",
    "    plot(deltax, yk1*ones(length(deltax)), color=\"black\", linestyle=\"--\")\n",
    "    plot(xk1*ones(length(deltay)), deltay, color=\"black\", linestyle=\"--\")\n",
    "    xk, yk = xk1, yk1\n",
    "end\n",
    "# Apresenta o ponto final em cyan\n",
    "plot(xk, yk, marker=\"o\", color=\"cyan\")\n",
    "show()\n",
    "println(\"Ponto final calculado (\", xk,\", \", yk, \")\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "É interessante brincar um pouco com o código acima variando o número de passos dados e o ponto de partida. Note também que o último ponto computado é impresso logo após o gráfico. Verifique como ele muda quando você muda os parâmetros sugeridos.\n",
    "\n",
    "Vemos então que a ideia simples de Jacobi pode funcionar, pelo menos em alguns casos. Será possível identificar de antemão se o método funcionará ou não? Isso será o tema da subseção Análise de Convergência abaixo. Por enquanto vamos fazer mais um experimento.\n",
    "\n",
    "O que ocorreria se o sistema tivesse as duas equações trocadas? Ou seja se ele fosse\n",
    "$$\n",
    "\\left\\{ \\begin{array}{rcrcl}\n",
    "x &-& 2y &=& -1 \\\\\n",
    "2.5x &-& y &=& 1.5. \n",
    "\\end{array}\\right.\n",
    "$$\n",
    "Nesse caso as fórmulas de atualização seriam\n",
    "\\begin{align*}\n",
    "x^{k+1} &= -1 + 2y^k \\\\\n",
    "y^{k+1} &= 2.5x - 1.5.\n",
    "\\end{align*}\n",
    "\n",
    "Podemos copiar a implementação do método acima com as devidas modificações para ver o que ocorre. Vamos agora marcar o ponto inicial na cor magenta para destacar um fenômeno interessante. Observe também que o ponto inicial está bem mais perto da solução. Ele é $(0.01, 0.01)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Método de Jacobi\n",
    "# Altere esses parametros para ver o novo comportamento.\n",
    "# Ponto incial\n",
    "xk, yk = 1.01, 1.01\n",
    "# Numero de iteracoes\n",
    "maxiter = 5\n",
    "\n",
    "# Apresenta o grafico inicial\n",
    "plot(x, eq1(x), color=\"blue\")\n",
    "plot(x, eq2(x), color=\"red\")\n",
    "\n",
    "# Apresenta o ponto de partida e as linhas que geram o novo ponto\n",
    "unsx = ones(length(x))\n",
    "y = 0.5:0.01:3.0\n",
    "unsy = ones(length(y))\n",
    "plot(xk, yk, marker=\"o\", color=\"magenta\")\n",
    "\n",
    "# Calcula os novo ponto e o apresenta\n",
    "for i = 1:maxiter\n",
    "    xk1 = -1.0 + 2.0*yk\n",
    "    yk1 = 2.5*xk - 1.5\n",
    "    deltax = xk1:-0.01:xk\n",
    "    deltay = yk1:-0.01:yk\n",
    "    plot(deltax, yk*ones(length(deltax)), color=\"black\", linestyle=\"--\")\n",
    "    plot(xk*ones(length(deltay)), deltay, color=\"black\", linestyle=\"--\")\n",
    "    plot(xk1, yk1, marker=\"o\", color=\"black\")\n",
    "    plot(deltax, yk1*ones(length(deltax)), color=\"black\", linestyle=\"--\")\n",
    "    plot(xk1*ones(length(deltay)), deltay, color=\"black\", linestyle=\"--\")\n",
    "    xk, yk = xk1, yk1\n",
    "end\n",
    "# Ponto final\n",
    "plot(xk, yk, marker=\"o\", color=\"cyan\")\n",
    "show()\n",
    "println(\"Ponto final calculado (\", xk,\", \", yk,\" )\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O que observamos acima é um pouco surpreendente. A sequencia gerada pelo Método de Jacobi agora se afasta da solução. Isso ocorreu devido a simples troca da ordem das equações, o que mostra que o método é bastante sensível. Vamos entender isso melhor na seção sobre convergência abaixo. Ainda, destacamos que apesar de o método ter sido apresentado para o caso de duas equações e duas variáveis, a sua extensão para o caso com $n$ equações e variáveis é direta. Basta isolar a variável $i$ usando a linha $i$ e obtemos a fórmula geral.\n",
    "\\begin{equation}\n",
    "x^{k+1}_i = \\frac{b_i - \\sum_{j = 1}^{i - 1} a_{ij} x^k_j - \\sum_{j = i + 1}^{n} a_{ij} x^k_j}{a_{ii}},\\ i = 1, \\ldots, n.\n",
    "\\end{equation}\n",
    "\n",
    "Podemos também intrduzir uma pequena variação que busca melhorar o método de Jacobi. Para isso vamos escrever as fórmulas genéricas de Jacobi para um sistema de 3 variáveis e 3 equações com matriz associada $A = (a_{ij})$.\n",
    "\\begin{align*}\n",
    "x^{k+1}_1 &= \\frac{b_1 - a_{12} x^k_2 - a_{13}x^k_3}{a_{11}} \\\\\n",
    "x^{k+1}_2 &= \\frac{b_2 - a_{21} x^k_1 - a_{23}x^k_3}{a_{22}} \\\\\n",
    "x^{k+1}_3 &= \\frac{b_3 - a_{31} x^k_1 - a_{32}x^k_2}{a_{33}}.\n",
    "\\end{align*}\n",
    "Observe que no momento que vamos calcular $x^{k+1}_3$ já temos as novas aproximações das duas primeiras coordenadas $x^{k + 1}_1$ e $x^{k + 1}_2$. Se o método estiver indo bem, temos a expectativas que essas aproximações sejam melhores que os valores de $x^k$. Então por que não aproveitá-los? Essa é a ideia do método de Gauss-Seidel. Nele as novas coordenadas já computadas são aproveitadas no cômputo da próxima coordenada. No caso de 3 variáveis teríamos:\n",
    "\\begin{align*}\n",
    "x^{k+1}_1 &= \\frac{b_1 - a_{12} x^k_2 - a_{13}x^k_3}{a_{11}} \\\\\n",
    "x^{k+1}_2 &= \\frac{b_2 - a_{21} x^{k+1}_1 - a_{23}x^k_3}{a_{22}} \\\\\n",
    "x^{k+1}_3 &= \\frac{b_3 - a_{31} x^{k+1}_1 - a_{32}x^{k+1}_2}{a_{33}}.\n",
    "\\end{align*}\n",
    "Podemos também apresentar a versão geral do método de Gauss-Seidel, adaptando a fórmula geral do método de Jacobi acima.\n",
    "\\begin{equation}\n",
    "x^{k+1}_i = \\frac{b_i - \\sum_{j = 1}^{i - 1} a_{ij} x^{k+1}_j - \\sum_{j = i + 1}^{n} a_{ij} x^k_j}{a_{ii}},\\ i = 1, \\ldots, n.\n",
    "\\end{equation}\n",
    "\n",
    "Abaixo modificamos a implementação anterior do método de Jacobi para o sistema do início desta seção de modo usar a ideia de Gauss-Seidel. Note que a aproximação da solução obtida após um número fixo de iterações é melhor do que o método de Jacobi. Isso ocorre devido ao uso de informação mais recente à medida que as coordenadas são calculadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Método de Gauss-Seidel\n",
    "# Altere esses parametros para ver o novo comportamento.\n",
    "# Ponto incial\n",
    "xk, yk = 1.8, 2.1\n",
    "# Numero de iteracoes\n",
    "maxiter = 5\n",
    "\n",
    "# Apresenta o grafico inicial\n",
    "plot(x, eq1(x), color=\"blue\")\n",
    "plot(x, eq2(x), color=\"red\")\n",
    "\n",
    "# Apresenta o ponto de partida e as linhas que geram o novo ponto\n",
    "unsx = ones(length(x))\n",
    "y = 0.5:0.01:3.0\n",
    "unsy = ones(length(y))\n",
    "plot(xk, yk, marker=\"o\", color=\"green\")\n",
    "\n",
    "# Calcula os novo ponto e o apresenta\n",
    "for i = 1:maxiter\n",
    "    xk1 = (1.5 + yk)/2.5\n",
    "    yk1 = (1.0 + xk1)/2.0\n",
    "    deltax = xk1:0.01:xk\n",
    "    deltay = yk1:0.01:yk\n",
    "    plot(deltax, yk*ones(length(deltax)), color=\"black\", linestyle=\"--\")\n",
    "    plot(xk1, yk1, marker=\"o\", color=\"black\")\n",
    "    plot(xk1*ones(length(deltay)), deltay, color=\"black\", linestyle=\"--\")\n",
    "    xk, yk = xk1, yk1\n",
    "end\n",
    "plot(xk, yk, marker=\"o\", color=\"cyan\")\n",
    "show()\n",
    "println(\"Ponto final calculado (\", xk,\", \", yk,\" )\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Porém o método sofre de problemas semelhantes ao de Jacobi. Uma simples troca da ordem das equações faz com que o método se afaste da solução ao invés de se aproximar. Vamos agora começar a estudar quando podemos garantir que um método converge à solução do problema.\n",
    "\n",
    "## Convergência\n",
    "\n",
    "Nessa seção vamos supor que a matriz do sistema\n",
    "$$\n",
    "Ax = b\n",
    "$$\n",
    "é inversível. Isso garante que o sistema tem solução única que vamos denotar por $x^*$. \n",
    "\n",
    "Como vimos, os métodos de Jacobi e Gauss-Seidel não tentam calcular uma solução diretamente. Eles tentam melhorar uma aproximação da solução cada vez mais, gerando uma sequencia $x^1, x^2, x^2, \\ldots$. Quando podemos dizer que a solução obtida é suficiente boa? Quando podemos dizer que um método desses funciona?\n",
    "\n",
    "**Definição.** Seja $x^1, x^2, x^2, \\ldots$, uma sequência gerada por um método iterativo. Dizemos que o método *converge* se existe $x^*$ solução do problema de interesse tal que a sequencia calculada $\\{ x^k \\}$ converge para $x^*$. Ou seja se a distância entre $x^k$ e $x^*$ converge para 0 (zero).\n",
    "\n",
    "Nosso objetivo agora é apresentar condições que possam garantir que os métodos iterativos que estudamos convergem à solução $x^*$ do sistema $Ax = b$. Para isso vamos começar observando que uma forma interessante de se ver um método iterativo é como uma função que é calculada em uma aproximação $x$ do ponto desejado resultando em uma nova aproximação $x^+$ que é potencialmente melhor. Ou seja, podemos imaginar que um algoritmo pode muitas vezes ser descrito por uma função $\\phi: \\mathbb{R}^n \\rightarrow \\mathbb{R}^n$ e a regra\n",
    "$$\n",
    "x^{k + 1} = \\phi(x^k).\n",
    "$$\n",
    "Caso tal função de iteração $\\phi$ exista, é natural que ela tenha a propriedade de que se o ponto de partida já é a solução $x^*$ então a $\\phi$ diga que \"se deve ficar parado\" para não perder a solução. Isto é\n",
    "$$\n",
    "x^* = \\phi(x^*).\n",
    "$$\n",
    "Além disso, também é natural pedir que se o ponto não for uma solução então $\\phi$ devolva um ponto diferente. Caso contrário método iterativo poderia ficar parado em cima de pontos que não são soluções. Isso em linguagem matemática é o mesmo que dizer que $\\phi$ deve ter como único ponto fixo justamente a solução do problema de interesse.\n",
    "\n",
    "Retomando o problema de sistemas lineares, vamos tentar re-escrever os métodos de Jacobi e Gauss-Seidel descobrindo a expressão da função $\\phi$. Para isso é útil quebrar a matriz $A$ do sistema que desejamos resolver\n",
    "$$\n",
    "A x = b\n",
    "$$\n",
    "em três submatrizes. Vamos escrever $A = L + D + U$, em que $L$ contém os elementos abaixo da diagonal de $A$ (e tem zero na diagonal e acima dela), $D$ contém a diagonal de $A$ (e zero fora da diagonal) e $U$ possui os elementos que ficam acima da diagonal.\n",
    "\n",
    "Retomando a formula do Jacobi.\n",
    "\\begin{equation*}\n",
    "x^{k+1}_i = \\frac{b_i - \\sum_{j = 1}^{i - 1} a_{ij} x^k_j - \\sum_{j = i + 1}^{n} a_{ij} x^k_j}{a_{ii}},\\ i = 1, \\ldots, n,\n",
    "\\end{equation*}\n",
    "podemos escrevê-la de forma mais compacta como\n",
    "$$\n",
    "x^{k + 1} = D^{-1}(b - Lx^k - U x^k).\n",
    "$$\n",
    "Reorganizando os termos temos\n",
    "$$\n",
    "x^{k + 1} = -D^{-1}(L + U)x^k + D^{-1}b.\n",
    "$$\n",
    "Isso sugere a função de iteração\n",
    "$$\n",
    "\\phi_J(x) = -D^{-1}(L + U)x + D^{-1}b.\n",
    "$$\n",
    "No caso do método de Gauss-Seidel a situação é semelhante, porém um pouco mais interessante.\n",
    "\\begin{gather}\n",
    "x^{k + 1} = D^{-1}(b - Lx^{k+1} - U x^k) \\iff \\\\\n",
    "D x^{k + 1} = b - Lx^{k+1} - U x^k \\iff \\\\\n",
    "D x^{k + 1} + L x^{k + 1} = -Ux^k + b \\iff \\\\\n",
    "(D + L) x^{k + 1} = -Ux^k + b \\iff \\\\\n",
    "x^{k + 1} = -(D + L)^{-1}Ux^k + (D + L)^{-1}b.\n",
    "\\end{gather}\n",
    "Nesse caso a função de iteração é \n",
    "$$\n",
    "\\phi_{GS}(x) = -(D + L)^{-1}Ux + (D + L)^{-1}b.\n",
    "$$\n",
    "\n",
    "Nos dois casos vemos que a função de iteração pode ser escrita como \n",
    "$$\n",
    "\\phi(x) = Bx + c,\n",
    "$$ \n",
    "em que $B$ é uma matriz e c um vetor constante. Também nos dois casos como $x^*$ é tal que $A x^* = b$ temos $\\phi(x^*) = x^*$, como gostaríamos. Vamos ver isso no caso de Gauss-Seidel\n",
    "\\begin{align*}\n",
    "\\phi_{GS}(x^*) &= -(D + L)^{-1}Ux^* + (D + L)^{-1}b \\\\\n",
    "               &= -(A - U)^{-1}Ux^* + (A - U)^{-1}b \\\\\n",
    "               &= -(A - U)^{-1}Ux^* + (A - U)^{-1} A x^* \\\\\n",
    "               &= (A - U)^{-1}(-U + A)x^* \\\\\n",
    "               &= x^*.\n",
    "\\end{align*}\n",
    "A pergunta sobre a convergência dos métodos pode então ser repensada da seguinte forma: se $\\phi(x)$ tem a expressão $Bx + c$, quando podemos garantir que o $\\| x^k - x^* \\|$ converge para $0$? Para isso façamos algumas manipulações simples\n",
    "\\begin{align*}\n",
    "\\| x^{k + 1} - x^* \\|_\\dagger &= \\| \\phi(x^k) - x^* \\|_\\dagger \\\\\n",
    "                              &= \\| \\phi(x^k) - \\phi(x^*) \\|_\\dagger \\\\\n",
    "                              &= \\| Bx^k + c - Bx^* - c \\|_\\dagger \\\\\n",
    "                              &= \\| B(x^k - x^*) \\|_\\dagger \\\\\n",
    "                              &\\leq \\| B \\|_\\dagger \\| x^k - x^* \\|_\\dagger.\n",
    "\\end{align*}\n",
    "Na última passagem usamos a propriedade que relaciona a norma de matrizes com a respectiva norma de vetores.\n",
    "\n",
    "Aplicando isso recursivamente vemos que \n",
    "$$\n",
    "\\| x^{k + 1} - x^* \\|_\\dagger \\leq  \\| B \\|_\\dagger^k \\| x^1 - x^* \\|_\\dagger.\n",
    "$$\n",
    "Note que isso garante que $\\| x^{k + 1} - x^* \\|_\\dagger \\rightarrow 0$ sempre que $\\| B \\|_\\dagger < 1$. Podemos imediatamente enunciar o seguinte resultado:\n",
    "\n",
    "**Teorema.** Considere que um sistema linear $Ax = b$ com solução única. Considere também um método iterativo descrito por uma função $\\phi$ que tenha a solução do sistema como único ponto fixo. Se $\\phi$ é descrita por $\\phi(x) = Bx + c$ então o método converge sempre que $\\| B \\|_\\dagger < 1$ para alguma das normas consideradas.\n",
    "\n",
    "Obs: Note que acima não escrevemos \"converge na norma $\\| \\cdot \\|_\\dagger$\" mas simplesmente usamos \"converge\". Isso porque é fácil de provar, pelo menos paras as normas vistas nessa seção que se $\\| x^k - x^* \\|_\\dagger \\rightarrow 0$ para alguma das normas o mesmo ocorre para as outras. Isso ocorre porque é possível achar para cada par de normas uma constante, que geralmente depende da dimensão do espaço, tal que uma norma é menor do que essa constante vezes a outra.\n",
    "\n",
    "O que esse teorema nos ensina sobre os métodos de Jacobi e Gauss-Seidel? Por exemplo, no caso do método de Jacobi, em que a matriz $B = -D^{-1}(L + U)$ podemos ver que\n",
    "\n",
    "**Teorema.** A matriz associada ao método de Jacobi $B = -D^{-1}(L + U)$ tem norma infinito menor estrita que $1$ se \n",
    "$$\n",
    "| a_{ii} | > \\sum_{j = 1, j \\neq i}^n | a_{ij} |.\n",
    "$$\n",
    "**Prova.** Lembremos que a norma infinito de uma matriz é a máxima norma 1 de suas linhas. Mas os elementos da linha $i$ de $B$ são exatamente \n",
    "$$\n",
    "b_{ij} = \\begin{cases} \\frac{a_{ij}}{a_{ii}},\\ j = 1, \\ldots, n, I \\neq j \\\\ 0,\\ i = j. \\end{cases}\n",
    "$$\n",
    "Assim, a norma 1 da linha $i$ de b é igual a\n",
    "$$\n",
    "\\sum_{\\substack{j = 1\\\\ i \\neq j}}^n \\left| \\frac{a_{ij}}{a_{ii}} \\right| < 1.\n",
    "$$\n",
    "A última desigualdade segue imediatamente da hipótese do teorema. Ou seja, todas as linhas de $B$ tem norma $1$ menor que $1$ e portanto $\\| B \\|_\\infty < 1$. $\\blacksquare$\n",
    "\n",
    "Obs: Uma matriz cujos elementos da diagonal superam em módulo somas dos módulos dos outros elementos da mesma linha é chamada de *matriz diagonal dominante por linhas*. O que o teorema diz é que o método de Jacobi converge se a matriz do sistema for diagonal dominante por linhas. Observe que esse resultado ajuda a entender porque a ordem das equações, que estão relacionadas às linhas da matriz, é importante para a convergência. Se a linha escolhilda para isolar o termo $x_i$ tiver a constante que multiplica essa variável muito pequena em módulo o método pode não convergir. Isso é exatamente o que ocorre quando trocamos as ordem das duas equações no sistema usando como exemplo anteriormente.\n",
    "\n",
    "Acabamos essa seção com dois comentários e com um exemplo de implementação do método de Jacobi.\n",
    "\n",
    "1. Pode-se mostrar que o critério de dominância por linhas também é válido para Gauss-Seidel.\n",
    "\n",
    "1. O método de Gauss-Seidel é tipicamente mais rápido do que o de Jacobi. Porém ele estabelece uma ordem na qual as variáveis devem ser atualizadas e por isso é de paralelização mais difícil. Nesse sentido há problemas em que Jacobi ainda pode valer à pena se o ganho com seu palelismo inerente foi maior do que o ganho obtido por Gauss-Seidel por aproveitar os valores das vairáveis já atualizadas. Isso tem se tornado mais importante nos últimos anos em que o crescimento de poder computacional tem vindo mais do aumento do número de processadores do que no aumento de velocidade de cada unidade de processamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementacao inocente do método de Jacobi\n",
    "function jacobi(A, b, maxiters=100, prec=1.0e-5)\n",
    "    # Pega a dimensão do problema\n",
    "    n = length(b)\n",
    "    \n",
    "    # Constroi as matrizes de interação\n",
    "    Dinv = 1.0 ./ diag(A)\n",
    "    LpU = copy(A)\n",
    "    for i = 1:n\n",
    "        LpU[i, i] = 0.0\n",
    "    end\n",
    "    \n",
    "    # Inicialização\n",
    "    iters = 0\n",
    "    c = Dinv .* b\n",
    "    x = copy(c)\n",
    "    \n",
    "    # Iterações de Jacobi\n",
    "    @printf \"Iter %d, Resíduo = %.2e\\n\" iters norm(A*x - b)\n",
    "    while iters < maxiters && norm(A*x - b) > prec\n",
    "        x = -Dinv .* (LpU*x) + c\n",
    "        iters += 1\n",
    "        @printf \"Iter %d, Resíduo = %.2e\\n\" iters norm(A*x - b)\n",
    "    end\n",
    "    return x\n",
    "end\n",
    "\n",
    "\n",
    "# Dimensão do problema de teste.\n",
    "n = 1000\n",
    "# Pega matriz e lado direitos aleatórios\n",
    "A = rand(n, n)\n",
    "b = rand(n)\n",
    "# Faz a matriz ficar diagonal dominante\n",
    "for i = 1:n\n",
    "    A[i, i] += 1001.0\n",
    "end\n",
    "\n",
    "# Testa o método\n",
    "x = jacobi(A, b)\n",
    "@show norm(A*x - b);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "author": "Paulo J. S. Silva",
  "kernelspec": {
   "display_name": "Julia 1.6.2",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
